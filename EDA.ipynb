{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning + Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime, timedelta\n",
    "import pandas as pd\n",
    "import holidays\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeTzRows(data, dtList): \n",
    "    \"\"\"remove rows of datetimes that do not exist due to tz changes.\"\"\"\n",
    "    for d in dtList:\n",
    "        if d in data.index:\n",
    "            data.drop(d, inplace=True)\n",
    "            print(str(d) + \" dropped.\")  \n",
    "            \n",
    "    return data\n",
    "\n",
    "\n",
    "def newIndex(start, end, delta):\n",
    "    \"\"\"create a new index for reindexing.\"\"\"\n",
    "    curr = start\n",
    "    while curr < end:\n",
    "        yield curr\n",
    "        curr += delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "startDt = datetime(2017, 1, 1, 0, 0)\n",
    "endDt = datetime(2020, 1, 1, 0, 0)\n",
    "# datetimes that should not exist due to tz changes\n",
    "tzDates = [datetime(2017,3,12,2,0), datetime(2017,3,12,2,30),\n",
    "            datetime(2018,3,11,2,0), datetime(2018,3,11,2,30),\n",
    "            datetime(2019,3,10,2,0), datetime(2019,3,10,2,30)\n",
    "           ]\n",
    "usHolidays = holidays.UnitedStates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepData(data, delta=30):\n",
    "    # some zones have missing values\n",
    "    # that are supposed to be zero.\n",
    "    data.fillna(0, axis=1, inplace=True)\n",
    "    \n",
    "    # drop the hour between 2 and 3 \n",
    "    # when US time goes so summer time\n",
    "    data = removeTzRows(data, tzDates)\n",
    "    \n",
    "    # drop duplicate index rows\n",
    "    duplicateNumber = data.index.duplicated().astype(int).sum()\n",
    "    data = data.loc[~data.index.duplicated(keep='first')]\n",
    "    print(str(duplicateNumber) + \" duplicates found and removed.\")\n",
    "    \n",
    "    # create new index with 30 minute time interval\n",
    "    reindx = list()\n",
    "    for result in newIndex(startDt, endDt, timedelta(minutes=delta)):\n",
    "        reindx.append(result)\n",
    "        \n",
    "    # reindex so there are no missing datetimes\n",
    "    data = data.reindex(reindx)\n",
    "    \n",
    "    # fill missing values with their previous one\n",
    "    data.fillna(method=\"ffill\", inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, set index as type datetime\n",
    "pu = pd.read_csv(\"zonePickups.csv\", index_col=\"time_bin\", parse_dates=[\"time_bin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-03-12 02:00:00 dropped.\n",
      "2017-03-12 02:30:00 dropped.\n",
      "2018-03-11 02:30:00 dropped.\n",
      "0 duplicates found and removed.\n"
     ]
    }
   ],
   "source": [
    "pu = prepData(pu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list containing all zone IDs\n",
    "zoneIds = pu.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Adding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addCalendarFeatures(data):\n",
    "    # add year, month, hour and weekday\n",
    "    data[\"dt\"] = data.index\n",
    "    data[\"year\"] = data[\"dt\"].apply(lambda x: x.year)\n",
    "    data[\"month\"] = data[\"dt\"].apply(lambda x: x.month)\n",
    "    data[\"hour\"] = data[\"dt\"].apply(lambda x: x.hour)\n",
    "    data[\"weekday\"] = data[\"dt\"].apply(lambda x: date(x.year, x.month, x.day).weekday())\n",
    "    \n",
    "    # add dummy for if a day is an US holiday\n",
    "    data[\"holiday\"] = data[\"dt\"].apply(lambda x: x in usHolidays).astype(int)\n",
    "    \n",
    "    # drop helper column dt\n",
    "    data.drop(\"dt\", axis=1, inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "pu = addCalendarFeatures(pu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadWeatherData(path, tarTimezone=\"US/Eastern\", dateFormat=\"%Y-%m-%d %H:%M\"):\n",
    "    \"\"\"Read weather data, transform datetime to target timezone, set as index.\n",
    "\n",
    "    Keyword arguments:\n",
    "    path -- path of weather data with data type\n",
    "    tarTimezone -- target timezone date is transformed into\n",
    "    dateFormat -- format of datetime\n",
    "    \"\"\"\n",
    "    w = pd.read_csv(path)\n",
    "    eastern = pytz.timezone(tarTimezone)\n",
    "    fmt = dateFormat\n",
    "    w[\"dt_iso\"] = w[\"dt_iso\"].apply(lambda x: pd.to_datetime(x[:-10], utc=True))\n",
    "    w[\"dt\"] = w[\"dt_iso\"].apply(lambda x: x.astimezone(eastern).strftime(fmt))\n",
    "    w[\"dt\"] = w[\"dt\"].apply(lambda x: pd.to_datetime(x))\n",
    "    w = w.set_index(\"dt\")\n",
    "    \n",
    "    return w[[\"temp\", \"feels_like\", \"temp_min\", \"temp_max\", \"humidity\", \"wind_speed\", \"weather_main\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = loadWeatherData(\"weather.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322 duplicates found and removed.\n"
     ]
    }
   ],
   "source": [
    "w = prepData(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join data with weather data\n",
    "pu = pu.join(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['year', 'month', 'hour', 'weekday', 'holiday', 'temp', 'feels_like',\n",
       "       'temp_min', 'temp_max', 'humidity', 'wind_speed', 'weather_main'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if all columns are joined correctly\n",
    "pu.drop(columns=zoneIds).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if every day has 48 entries\n",
    "len(pu) % 48 == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "pu.to_csv(\"zonePickupsFinal.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allInOne(path, weatherPath=\"weather.csv\", saveFileName=None):\n",
    "    # load data, set index as type datetime\n",
    "    data = pd.read_csv(path, index_col=\"time_bin\", parse_dates=[\"time_bin\"])\n",
    "    \n",
    "    data = prepData(data)\n",
    "    \n",
    "    # create a list containing all zone IDs\n",
    "    zoneIds = data.columns.tolist()\n",
    "    \n",
    "    data = addCalendarFeatures(data)\n",
    "    \n",
    "    w = loadWeatherData(weatherPath)\n",
    "    w = prepData(w)\n",
    "    \n",
    "    # join data with weather data\n",
    "    data = data.join(w)\n",
    "    \n",
    "    # check if all columns are joined correctly\n",
    "    cols = data.drop(columns=zoneIds).columns\n",
    "    print(\"columns ex zones:\")\n",
    "    for col in cols:\n",
    "        print(col)\n",
    "        \n",
    "    # check if every day has 48 entries\n",
    "    if len(pu) % 48 == 0:\n",
    "        print(\"No odd rows.\")\n",
    "    else:\n",
    "        print(\"There might be odd/duplicate index rows. Check!\")\n",
    "        \n",
    "    if saveFileName != None:\n",
    "        data.to_csv(saveFileName)\n",
    "        print(\"Saved as \" + saveFileName)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = allInOne(\"zonePickups.csv\", saveFileName=\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
