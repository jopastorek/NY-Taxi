{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime, timedelta, time\n",
    "import holidays\n",
    "import pytz\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "helper functions for data retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeTzRows(data, dtList): \n",
    "    \"\"\"remove rows of datetimes that do not exist due to tz changes.\"\"\"\n",
    "    for d in dtList:\n",
    "        if d in data.index:\n",
    "            data.drop(d, inplace=True)\n",
    "            print(str(d) + \" dropped.\")  \n",
    "    return data\n",
    "def newIndex(start, end, delta):\n",
    "    \"\"\"create a new index for reindexing.\"\"\"\n",
    "    curr = start\n",
    "    while curr < end:\n",
    "        yield curr\n",
    "        curr += delta\n",
    "        \n",
    "def prepData(data, delta=60):\n",
    "    \"\"\"removes duplicates that occur due to DST changes\"\"\"\n",
    "    # some zones have missing values\n",
    "    # that are supposed to be zero.\n",
    "    data.fillna(0, axis=1, inplace=True)\n",
    "    # drop the hour between 2 and 3 \n",
    "    # when US time goes so summer time\n",
    "    data = removeTzRows(data, tzDates)\n",
    "    # drop duplicate index rows\n",
    "    duplicateNumber = data.index.duplicated().astype(int).sum()\n",
    "    data = data.loc[~data.index.duplicated(keep='first')]\n",
    "    print(str(duplicateNumber) + \" duplicates found and removed.\")\n",
    "    # create new index with 30 minute time interval\n",
    "    reindx = list()\n",
    "    for result in newIndex(startDt, endDt, timedelta(minutes=delta)):\n",
    "        reindx.append(result)\n",
    "    # reindex so there are no missing datetimes\n",
    "    data = data.reindex(reindx)\n",
    "    # fill missing values with their previous one\n",
    "    data.fillna(method=\"ffill\", inplace=True)\n",
    "    return data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* start and end of observation horizon\n",
    "* tzDates: datetimes that do not exist due to DST changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "startDt = datetime(2017, 1, 1, 0, 0)\n",
    "endDt = datetime(2020, 1, 1, 0, 0)\n",
    "# datetimes that should not exist due to tz changes\n",
    "tzDates = [datetime(2017,3,12,2,0), datetime(2017,3,12,2,30),\n",
    "            datetime(2018,3,11,2,0), datetime(2018,3,11,2,30),\n",
    "            datetime(2019,3,10,2,0), datetime(2019,3,10,2,30)\n",
    "           ]\n",
    "usHolidays = holidays.UnitedStates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load zone details\n",
    "zones = pd.read_csv('../taxi_data/taxiZoneLookup.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregated Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* aggregates trips for every dt for each zone\n",
    "* time-series format\n",
    "* used to explore trips more easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggr_data(years):\n",
    "    check = 1\n",
    "    for year in years:\n",
    "        for month in range(1,13):\n",
    "            path = \"../taxi_data/yellow_tripdata_{}-{}{}\".format(year, f\"{month:02d}\", \".csv\")\n",
    "            df = pd.read_csv(path, \n",
    "                             parse_dates=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"], \n",
    "                             low_memory=False)\n",
    "            \n",
    "            df = df[[\"tpep_pickup_datetime\", \n",
    "                     \"tpep_dropoff_datetime\", \n",
    "                     \"trip_distance\", \n",
    "                     \"passenger_count\", \n",
    "                     \"PULocationID\", \n",
    "                     \"DOLocationID\"]]\n",
    "            \n",
    "            #only trips shorter than 100 miles\n",
    "            df = df[df.trip_distance < 100]\n",
    "            #only trips with at least 1 passenger\n",
    "            df = df[df.passenger_count != 0]\n",
    "            #calculate trip duration\n",
    "            df[\"trip_duration\"] = df[\"tpep_dropoff_datetime\"] - df[\"tpep_pickup_datetime\"]\n",
    "            #only trips longer than 1 minute and shorter than 3 hours\n",
    "            df = df[(df.trip_duration > timedelta(minutes=1)) & (df.trip_duration < timedelta(hours=3))]\n",
    "            \n",
    "            #merge borough from the zones dataframe\n",
    "            #so trips can be filtered for Manhattan, Queens and Brooklyn only\n",
    "            df.rename(columns={\"PULocationID\": \"LocationID\"}, inplace=True)\n",
    "            df = pd.merge(df, zones[[\"LocationID\", \"Borough\"]], on=\"LocationID\", how=\"left\")\n",
    "            df.rename(columns={\"LocationID\": \"PULocationID\", \"Borough\": \"PUBorough\", \"Zone\": \"PUZone\"}, inplace=True)\n",
    "            df.rename(columns={\"DOLocationID\": \"LocationID\"}, inplace=True)\n",
    "            df = pd.merge(df, zones[[\"LocationID\", \"Borough\"]], on=\"LocationID\", how=\"left\")\n",
    "            df.rename(columns={\"LocationID\": \"DOLocationID\", \"Borough\": \"DOBorough\", \"Zone\": \"DOZone\"}, inplace=True)\n",
    "            \n",
    "            #only include trips from and to Manhattan, Queens or Brooklyn\n",
    "            boroughs = [\"Manhattan\", \"Queens\", \"Brooklyn\"]\n",
    "            df = df[(df.PUBorough.isin(boroughs)) & (df.DOBorough.isin(boroughs))]\n",
    "            \n",
    "            #drop everything except datetime information and pickup location\n",
    "            #df = df[[\"tpep_pickup_datetime\", \"PULocationID\"]]\n",
    "            df = df[[\"tpep_dropoff_datetime\", \"DOLocationID\"]]\n",
    "            #only include trips that fill in the current year and month\n",
    "            if month != 12:\n",
    "                df = df[(df.tpep_dropoff_datetime >= datetime(year,month,1)) & (df.tpep_dropoff_datetime < datetime(year,month+1,1))]\n",
    "            else:\n",
    "                df = df[(df.tpep_dropoff_datetime >= datetime(year,month,1)) & (df.tpep_dropoff_datetime < datetime(year+1,1,1))]\n",
    "            \n",
    "            #round pickup datetime to full hours\n",
    "            df[\"tpep_dropoff_datetime\"] = df[\"tpep_dropoff_datetime\"].apply(lambda x: x - timedelta(minutes=x.minute % 60, seconds=x.second))    \n",
    "            #create a dummy for each zone\n",
    "            df = df[[\"tpep_dropoff_datetime\"]].join(pd.get_dummies(df.DOLocationID))\n",
    "            #aggregate trips for every zone at every pickup datetime\n",
    "            df = df.groupby(\"tpep_dropoff_datetime\").sum()    \n",
    "            \n",
    "            #aggregate data unless it's the first data set (here 1/2017)\n",
    "            if check == 0:\n",
    "                final = pd.concat([final, df], sort=False)\n",
    "            else:\n",
    "                final = df\n",
    "                check = 0\n",
    "                \n",
    "            print(str(year) + \"-\" + str(month))\n",
    "            \n",
    "    return final          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2017,2018,2019]\n",
    "#df1 = get_aggr_data(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1.to_csv(\"../taxi_data/aggr_dropoffs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"../taxi_data/aggr_pickups.csv\", \n",
    "                  index_col=[0], \n",
    "                  parse_dates=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 duplicates found and removed.\n"
     ]
    }
   ],
   "source": [
    "#remove duplicates or dts that do not exist due to summertime/wintertime shifts\n",
    "df1 = prepData(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.to_csv(\"taxi_data/aggr_pickups.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>4</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>...</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-12-31 19:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>273.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 20:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>422.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>326.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 22:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>225.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 23:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>164.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        2     4     7    8    9   10   11   12    13   14  \\\n",
       "tpep_pickup_datetime                                                        \n",
       "2019-12-31 19:00:00   0.0  18.0  13.0  1.0  0.0  0.0  0.0  0.0  87.0  2.0   \n",
       "2019-12-31 20:00:00   0.0  25.0  24.0  0.0  0.0  1.0  0.0  0.0  86.0  1.0   \n",
       "2019-12-31 21:00:00   0.0  22.0  20.0  0.0  0.0  2.0  1.0  1.0  62.0  2.0   \n",
       "2019-12-31 22:00:00   0.0  31.0  16.0  1.0  0.0  2.0  0.0  1.0  41.0  0.0   \n",
       "2019-12-31 23:00:00   0.0  27.0  17.0  0.0  0.0  3.0  0.0  0.0  28.0  0.0   \n",
       "\n",
       "                      ...  252  253   255   256  257  258  260   261    262  \\\n",
       "tpep_pickup_datetime  ...                                                     \n",
       "2019-12-31 19:00:00   ...  0.0  0.0   4.0   7.0  1.0  0.0  4.0  67.0  196.0   \n",
       "2019-12-31 20:00:00   ...  0.0  0.0  13.0  13.0  2.0  0.0  7.0  75.0  211.0   \n",
       "2019-12-31 21:00:00   ...  0.0  0.0   4.0   5.0  1.0  1.0  8.0  53.0  168.0   \n",
       "2019-12-31 22:00:00   ...  0.0  0.0   6.0  11.0  1.0  2.0  9.0  31.0   85.0   \n",
       "2019-12-31 23:00:00   ...  0.0  0.0  12.0  17.0  1.0  0.0  3.0  32.0   69.0   \n",
       "\n",
       "                        263  \n",
       "tpep_pickup_datetime         \n",
       "2019-12-31 19:00:00   273.0  \n",
       "2019-12-31 20:00:00   422.0  \n",
       "2019-12-31 21:00:00   326.0  \n",
       "2019-12-31 22:00:00   225.0  \n",
       "2019-12-31 23:00:00   164.0  \n",
       "\n",
       "[5 rows x 198 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for Forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* raw data which will be the base for predictions later\n",
    "* data is accumulated such that every dt has an aggregated number of trips for each zone\n",
    "* no time-series format, but all zones can be included in one model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(years, pu, do, path, only_pu=1):\n",
    "    check = 1\n",
    "    for year in years:\n",
    "        #print(year)\n",
    "        for month in range(2,13):\n",
    "            date_str = str(year)+\"-\"+str(month)\n",
    "            print(date_str)\n",
    "            \n",
    "            path = \"../taxi_data/{}_{}-{}{}\".format(path, year, f\"{month:02d}\", \".csv\")\n",
    "            try:\n",
    "                df = pd.read_csv(path, \n",
    "                                 parse_dates=[pu, do], \n",
    "                                 #index_col=[\"tpep_pickup_datetime\"], \n",
    "                                 #low_memory=False\n",
    "                                )\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            start = datetime(year,month,1)\n",
    "            if month != 12:\n",
    "                end = datetime(year,month+1,1)\n",
    "            else:\n",
    "                end = datetime(year+1,1,1)\n",
    "            df = df[(df[pu]>=start) & (df[pu]<end)]\n",
    "            df = df[[pu, do,\n",
    "                     #\"trip_distance\", \n",
    "                     #\"passenger_count\",\n",
    "                     \"PULocationID\",\n",
    "                     \"DOLocationID\"]]\n",
    "            \n",
    "            #df = df[df.trip_distance < 100]\n",
    "            #df = df[df.passenger_count != 0]\n",
    "            #df[\"trip_duration\"] = df[do] - df[pu]\n",
    "            #df = df[(df.trip_duration > timedelta(minutes=1)) & (df.trip_duration < timedelta(hours=3))]\n",
    "            \n",
    "            #merge borough from the zones dataframe\n",
    "            #so trips can be filtered for Manhattan, Queens and Brooklyn only\n",
    "            df.rename(columns={\"PULocationID\": \"LocationID\"}, inplace=True)\n",
    "            df = pd.merge(df, zones[[\"LocationID\", \"Borough\"]], on=\"LocationID\", how=\"left\")\n",
    "            df.rename(columns={\"LocationID\": \"PULocationID\", \n",
    "                               \"Borough\": \"PUBorough\", \n",
    "                               \"Zone\": \"PUZone\"}, \n",
    "                      inplace=True)\n",
    "            \n",
    "            df.rename(columns={\"DOLocationID\": \"LocationID\"}, inplace=True)\n",
    "            df = pd.merge(df, zones[[\"LocationID\", \"Borough\"]], on=\"LocationID\", how=\"left\")\n",
    "            df.rename(columns={\"LocationID\": \"DOLocationID\", \n",
    "                               \"Borough\": \"DOBorough\", \n",
    "                               \"Zone\": \"DOZone\"}, \n",
    "                      inplace=True)\n",
    "                                    \n",
    "            #only include trips from and to Manhattan, Queens or Brooklyn\n",
    "            boroughs = [\"Manhattan\", \"Queens\", \"Brooklyn\"]\n",
    "            df = df[(df.PUBorough.isin(boroughs)) & (df.DOBorough.isin(boroughs))]\n",
    "                                          \n",
    "            if only_pu == 1:\n",
    "                cols = [pu, \"PULocationID\"]\n",
    "            else:\n",
    "                cols = [do, \"DOLocationID\"]\n",
    "            \n",
    "            df = df[cols]\n",
    "            df = df.rename(columns={pu: \"datetime\", cols[1]: \"location\"})\n",
    "            df[\"year\"] = df[\"datetime\"].apply(lambda x: x.year)\n",
    "            df[\"month\"] = df[\"datetime\"].apply(lambda x: x.month)\n",
    "            df[\"day\"] = df[\"datetime\"].apply(lambda x: x.day)\n",
    "            df[\"hour\"] = df[\"datetime\"].apply(lambda x: x.hour)\n",
    "            #df[\"wday\"] = df[\"tpep_pickup_datetime\"].apply(lambda x: x.weekday())\n",
    "            df = df.drop(\"datetime\", axis=1)\n",
    "            df[\"rides\"] = 0\n",
    "            df = df.groupby([\"location\", \"year\", \"month\", \"day\", \"hour\"]).agg(\"count\")\n",
    "            df.reset_index(inplace=True)  \n",
    "            if check == 0:\n",
    "                final = pd.concat([final, df], sort=False)\n",
    "            else:\n",
    "                final = df\n",
    "                check = 0\n",
    "            print(str(year) + \"-\" + str(month) + \" done!\")\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = get_data(years, \"pickup_datetime\", \"dropoff_datetime\", \"fhvhv_tripdata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.to_csv(\"taxi_data/dropoffs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* adjust data from UTC to NYC time (including daylist saving time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadWeatherData(path, tarTimezone=\"US/Eastern\", dateFormat=\"%Y-%m-%d %H:%M\"):\n",
    "    \"\"\"Read weather data, transform datetime to target timezone, set as index.\n",
    "    Keyword arguments:\n",
    "    path -- path of weather data with data type\n",
    "    tarTimezone -- target timezone date is transformed into\n",
    "    dateFormat -- format of datetime\n",
    "    \"\"\"\n",
    "    w = pd.read_csv(path)\n",
    "    eastern = pytz.timezone(tarTimezone)\n",
    "    fmt = dateFormat\n",
    "    w[\"dt_iso\"] = w[\"dt_iso\"].apply(lambda x: pd.to_datetime(x[:-10], utc=True))\n",
    "    w[\"dt\"] = w[\"dt_iso\"].apply(lambda x: x.astimezone(eastern).strftime(fmt))\n",
    "    w[\"dt\"] = w[\"dt\"].apply(lambda x: pd.to_datetime(x))\n",
    "    w = w.set_index(\"dt\")\n",
    "    return w[[\"temp\", \"feels_like\", \"temp_min\", \"temp_max\", \"humidity\", \"wind_speed\", \n",
    "              \"rain_1h\", \"rain_3h\", \"snow_1h\", \"snow_3h\", \"weather_main\", \"weather_description\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = loadWeatherData(\"../taxi_data/weather.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = w.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322 duplicates found and removed.\n"
     ]
    }
   ],
   "source": [
    "w = prepData(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.to_csv(\"../taxi_data/weather_prepared.csv\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:nf] *",
   "language": "python",
   "name": "conda-env-nf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
